{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "I have been working over the last couple of months to train an LLM on health and safety data. After fine-tuning, the model has been deployed to production and is being exposed to a front-end application through an API. I unfortunaly have neglacted to document the process of research and development and this repo will serve as a retrospective peice on my work. I will share the code to train the model and insights I learned along the way. Unfortunatly, since it is company IP, I cannot share the data or model itself, only my process for developing it.\n",
    "\n",
    "\n",
    "# Background and Motivation\n",
    "\n",
    "On construction sites, we have many hazards that pose health and safety threats to our workers or members of the public. To capture these hazards, our site workers use an app to log when they see an issue, such as: \"Cables left laying on walkway\", \"Oil spill on pathway\", etc. Thus, we have a huge amount of useful health and safety data stored in our database.\n",
    "\n",
    "When entering an observation, the user is given the option to enter a category. This is used by the Health and Saftey Team: to triage and action the hazard, to decide which team to send it to, whether to notify someone immediately, etc. For example, \"Cables left laying on walkway\" would probably be given the category \"Slips/Trips\". \n",
    "\n",
    "Most of the time the appropriate category is set in the data but around 18% of the time the category is empty and sometimes when it is set it isn't set to a value that matches the input text. This is where Machine Learning and LLMs can come in. With over 1 million records in the dataset we can use some filtering to pull out a subset of labelled records with good observations and fine-tune a model to learn which category is most likely set when certain words and combinations of words are present. We can then use this to predict/suggest a category and improve the process of submitting an observation as well as triaging and actioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we can load the data into a Dataframe in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Setting to fix bug later on in model.\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "observations = pd.read_csv(\"observations-data.csv\")\n",
    "display(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two key fields to pick out from the data. The one with the free text input, and the one with the category. The plan is to infer category2 to predict whatdidyousee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category2 as a Category1 is always \"Hazard\" due to the way the app stores the data.\n",
    "labelled = observations[[\"whatdidyousee\"], [\"category2\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a supervised learning model so we need to remove the null categories. This will give us a fully labelled Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled = labelled.loc[labelled[\"category2\"].notnull()]\n",
    "\n",
    "# Cleaning up labelled Dataset.\n",
    "labelled[\"category\"] = labelled[\"category2\"].str.strip()\n",
    "labelled = labelled.reset_index(drop=True)\n",
    "\n",
    "display(labelled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model we will need to have a sorted list of possible category options that we can later convert to a Tensor so our model can perform computations on it. Let's begin by getting a unique list and sorting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = [cat for cat in labelled[\"category\"].unique()]\n",
    "category_list.sort()\n",
    "print(category_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's graph the category distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled[\"category\"].value_counts()[category_list].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are happy with that let's store the category labels for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"observation_categories.json\", \"w\") as f:\n",
    "    json.dump({\"categories\": category_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin any training or data manipulation let's create an untouched cut of the data to do testing on later. This is important as we will run over multiple epochs when traingin and there will be some bias towards the validation set as the model will adjust it's predictions to reduce the loss on that dataset.\n",
    "\n",
    "Therefore we will need a sample of the data that the model has never seen before so we can properly evaluate it's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untouched = labelled.sample(10000)\n",
    "untouched.to_csv(\"observations-finaltest.csv\")\n",
    "\n",
    "# After saving the test data, remove it from the training dataset.\n",
    "labelled_remaining = labelled.drop(untouched.index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an untouched dataset for testing down the line we can start building splitting out our model data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 20000\n",
    "RANDOM_STATE = 200\n",
    "\n",
    "# Getting a sample of data\n",
    "model_data = labelled_remaining.sample(SAMPLE_SIZE,random_state=RANDOM_STATE)\n",
    "labelled_remaining = labelled_remaining.drop(model_data.index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting out all those datasets we can now validate they all look good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RAW: {observations.shape}\")\n",
    "print(f\"LABELLED: {labelled.shape}\")\n",
    "print(f\"LABELLED REMAINING: {labelled_remaining.shape}\")\n",
    "print(f\"MODEL DATA: {model_data.shape}\")\n",
    "print(f\"UNTOUCHED: {untouched.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that looks good then save the remaining for use in later Warm Start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_remaining.to_csv(\"observations-unseen.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: One-hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important first step in preparing the data for the model is called \"one-hot coding\". We need to do this as the model does not understand a term like \"Slip/Trip\" that might appear in the target category2 field. It won't be able to compute a loss function against that, to see how close or far it is from the correct label. \n",
    "\n",
    "To solve this in the multi-label classification context we need to encode the values using the sorted category list we prepared earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(category_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this we need to turn the target value e.g Slip/Trip into a tensor where the value at the Slip/Trip index is 1 and the value at all other categories is 0.\n",
    "\n",
    "As an example if there were three categories: Ant, Bee and Cricket then a labelled data point with the category \"Ant\" would become a target list of [1, 0, 0]. A category of \"Cricket\" would become [0, 0, 1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in category_list:\n",
    "    model_data.loc[model_data[\"category\"] == category, category] = 1\n",
    "    model_data[category] = model_data[category].fillna(0)\n",
    "\n",
    "model_data[\"input\"] = model_data[\"whatdidyousee\"].astype(str)\n",
    "\n",
    "pd.options.display.max_colwidth = 10\n",
    "display(model_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert our long set of columns into a single tensor for pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data[\"target_list\"] = model_data[category_list].astype(bool).values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's reassign the Dataframe to just have the key columns. To train we only need the input and the target_list but I'm including the category for better result readablility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = model_data[[\"input\", \"category\", \"target_list\"]]\n",
    "\n",
    "# Taking a look at the prepared dataset.\n",
    "pd.options.display.max_colwidth = 500\n",
    "display(model_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Train-Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a final testing set and a 20000 record dataset to train our model. But to give feedback during training it is best practise to split the model dataset into 2 seperate sets: Training and Validation.\n",
    "\n",
    "The idea is the model with loop over the training set in batches and repeatedly mark itself on it's prediction vs the output. Then the validation set is used as a check on the state of the model after training. If it scores highly on the validation set we know it has been generallised well on this cut of the observation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "train_dataset = model_data.sample(frac=TRAIN_SIZE)\n",
    "valid_dataset = model_data.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(f\"FULL Dataset: {model_data.shape}\")\n",
    "print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "print(f\"TEST Dataset: {valid_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Model Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we load in the bert tokenizer which will help with turning the raw text inputs into numbered tokens. We need to use the BERT tokenizer as the tokens will match with the BERT model we will load in later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The torch neural network we are going to use requires a particular object structure as an input. I will build this class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CustomDataset:\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.input = dataframe[\"input\"]\n",
    "        self.targets = self.data.target_list\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input = str(self.input[index])\n",
    "        input = \" \".join(input.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            input,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        token_type_ids = input[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"targets\": torch.tensor(self.targets[index], dtype=torch.float),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to create a small function to put the CustomDataset object into the DataLoader torch module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "MAX_LEN = 16\n",
    "\n",
    "def df_loader(df, batch_size):\n",
    "    custom = CustomDataset(df, tokenizer, MAX_LEN)\n",
    "    test_params = {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 0\n",
    "        }\n",
    "    return DataLoader(custom, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I will create a BertClass that inherits from the torch nerual network module. This class is our model. It is initially instatiated with the weights from BERT and will be fine-tuned over our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "class BERTClass(torch.nn.module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

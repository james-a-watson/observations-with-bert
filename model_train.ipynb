{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "I have been working over the last couple of months to train an LLM on health and safety data. After fine-tuning, the model has been deployed to production and is being exposed to a front-end application through an API. I unfortunaly have neglacted to document the process of research and development and this repo will serve as a retrospective peice on my work. I will share the code to train the model and insights I learned along the way. Unfortunatly, since it is company IP, I cannot share the data or model itself, only my process for developing it.\n",
    "\n",
    "\n",
    "# Background and Motivation\n",
    "\n",
    "On construction sites, we have many hazards that pose health and safety threats to our workers or members of the public. To capture these hazards, our site workers use an app to log when they see an issue. These observations are in free text form, such as: \"Cables left laying on walkway\", \"Oil spill on pathway\", \"Cement truck parked in wrong place\" etc. !here are over 1_000_000 of these records in the observations table. \n",
    "\n",
    "When entering an observation, the user is given the option to enter a category. This is used by the Health and Saftey Team: to triage and action the hazard, to decide which team to send it to, whether to notify someone immediately, etc. For example, \"Cables left laying on walkway\" would probably be given the category \"Slips/Trips\". \n",
    "\n",
    "Most of the time the appropriate category is set in the data but around 18% of the time the category is empty and sometimes when it is set it isn't set to a value that matches the input text. This is where Machine Learning and LLMs can come in. With over 1 million records in the dataset we can use some filtering to pull out a subset of labelled records with good observations and fine-tune a model to learn which category is most likely set when certain words and combinations of words are present. We can then use this to predict/suggest a category and improve the process of submitting an observation as well as triaging and actioning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start, we can load the data into a Dataframe in the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setting to fix bug later on in model.\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "observations = pd.read_csv(\"observations-data.csv\")\n",
    "display(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two key fields to pick out from the data. The one with the free text input, and the one with the category. The plan is to infer category2 to predict whatdidyousee."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category2 as a Category1 is always \"Hazard\" due to the way the app stores the data.\n",
    "labelled = observations[[\"whatdidyousee\"], [\"category2\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a supervised learning model so we need to remove the null categories. This will give us a fully labelled Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled = labelled.loc[labelled[\"category2\"].notnull()]\n",
    "\n",
    "# Cleaning up labelled Dataset.\n",
    "labelled[\"category\"] = labelled[\"category2\"].str.strip()\n",
    "labelled = labelled.reset_index(drop=True)\n",
    "\n",
    "display(labelled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To train our model we will need to have a sorted list of possible category options that we can later convert to a Tensor so our model can perform computations on it. Let's begin by getting a unique list and sorting it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_list = [cat for cat in labelled[\"category\"].unique()]\n",
    "category_list.sort()\n",
    "print(category_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's graph the category distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled[\"category\"].value_counts()[category_list].plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are happy with that let's store the category labels for later use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"observation_categories.json\", \"w\") as f:\n",
    "    json.dump({\"categories\": category_list})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we begin any training or data manipulation let's create an untouched cut of the data to do testing on later. This is important as we will run over multiple epochs when traingin and there will be some bias towards the validation set as the model will adjust it's predictions to reduce the loss on that dataset.\n",
    "\n",
    "Therefore we will need a sample of the data that the model has never seen before so we can properly evaluate it's accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "untouched = labelled.sample(10000)\n",
    "untouched.to_csv(\"observations-finaltest.csv\")\n",
    "\n",
    "# After saving the test data, remove it from the training dataset.\n",
    "labelled_remaining = labelled.drop(untouched.index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have an untouched dataset for testing down the line we can start building splitting out our model data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 20000\n",
    "RANDOM_STATE = 200\n",
    "\n",
    "# Getting a sample of data\n",
    "model_data = labelled_remaining.sample(SAMPLE_SIZE,random_state=RANDOM_STATE)\n",
    "labelled_remaining = labelled_remaining.drop(model_data.index).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting out all those datasets we can now validate they all look good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"RAW: {observations.shape}\")\n",
    "print(f\"LABELLED: {labelled.shape}\")\n",
    "print(f\"LABELLED REMAINING: {labelled_remaining.shape}\")\n",
    "print(f\"MODEL DATA: {model_data.shape}\")\n",
    "print(f\"UNTOUCHED: {untouched.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that looks good then save the remaining for use in later Warm Start training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labelled_remaining.to_csv(\"observations-unseen.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: One-hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important first step in preparing the data for the model is called \"one-hot coding\". We need to do this as the model does not understand a term like \"Slip/Trip\" that might appear in the target category2 field. It won't be able to compute a loss function against that, to see how close or far it is from the correct label. \n",
    "\n",
    "To solve this in the multi-label classification context we need to encode the values using the sorted category list we prepared earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(category_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this we need to turn the target value e.g Slip/Trip into a tensor where the value at the Slip/Trip index is 1 and the value at all other categories is 0.\n",
    "\n",
    "As an example if there were three categories: Ant, Bee and Cricket then a labelled data point with the category \"Ant\" would become a target list of [1, 0, 0]. A category of \"Cricket\" would become [0, 0, 1]. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category in category_list:\n",
    "    model_data.loc[model_data[\"category\"] == category, category] = 1\n",
    "    model_data[category] = model_data[category].fillna(0)\n",
    "\n",
    "model_data[\"input\"] = model_data[\"whatdidyousee\"].astype(str)\n",
    "\n",
    "pd.options.display.max_colwidth = 10\n",
    "display(model_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert our long set of columns into a single tensor for pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data[\"target_list\"] = model_data[category_list].astype(bool).values.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's reassign the Dataframe to just have the key columns. To train we only need the input and the target_list but I'm including the category for better result readablility."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = model_data[[\"input\", \"category\", \"target_list\"]]\n",
    "\n",
    "# Taking a look at the prepared dataset.\n",
    "pd.options.display.max_colwidth = 500\n",
    "display(model_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Train-Validation Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a final testing set and a 20000 record dataset to train our model. But to give feedback during training it is best practise to split the model dataset into 2 seperate sets: Training and Validation.\n",
    "\n",
    "The idea is the model with loop over the training set in batches and repeatedly mark itself on it's prediction vs the output. Then the validation set is used as a check on the state of the model after training. If it scores highly on the validation set we know it has been generallised well on this cut of the observation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "train_dataset = model_data.sample(frac=TRAIN_SIZE)\n",
    "valid_dataset = model_data.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "print(f\"FULL Dataset: {model_data.shape}\")\n",
    "print(f\"TRAIN Dataset: {train_dataset.shape}\")\n",
    "print(f\"TEST Dataset: {valid_dataset.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Model Prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where we load in the bert tokenizer which will help with turning the raw text inputs into numbered tokens. We need to use the BERT tokenizer as the tokens will match with the BERT model we will load in later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The torch neural network we are going to use requires a particular object structure as an input. I will build this class below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class CustomDataset:\n",
    "    def __init__(self, dataframe, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.data = dataframe\n",
    "        self.input = dataframe[\"input\"]\n",
    "        self.targets = self.data.target_list\n",
    "        self.max_len = max_len\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.input)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        input = str(self.input[index])\n",
    "        input = \" \".join(input.split())\n",
    "\n",
    "        inputs = self.tokenizer.encode_plus(\n",
    "            input,\n",
    "            None,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True,\n",
    "            truncation=True,\n",
    "        )\n",
    "        ids = inputs[\"input_ids\"]\n",
    "        mask = inputs[\"attention_mask\"]\n",
    "        token_type_ids = input[\"token_type_ids\"]\n",
    "\n",
    "        return {\n",
    "            \"ids\": torch.tensor(ids, dtype=torch.long),\n",
    "            \"mask\": torch.tensor(mask, dtype=torch.long),\n",
    "            \"token_type_ids\": torch.tensor(token_type_ids, dtype=torch.long),\n",
    "            \"targets\": torch.tensor(self.targets[index], dtype=torch.float),\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I'm going to create a small function to put the CustomDataset object into the DataLoader torch module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "MAX_LEN = 16\n",
    "\n",
    "def df_loader(df, batch_size):\n",
    "    custom = CustomDataset(df, tokenizer, MAX_LEN)\n",
    "    test_params = {\n",
    "        \"batch_size\": batch_size,\n",
    "        \"shuffle\": False,\n",
    "        \"num_workers\": 0\n",
    "        }\n",
    "    return DataLoader(custom, **test_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I will create a BertClass that inherits from the torch nerual network module. This class is our model. It is initially instatiated with the weights from BERT and will be fine-tuned over our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "class BERTClass(torch.nn.module):\n",
    "    def __init__(self):\n",
    "        super(BERTClass, self).__init__()\n",
    "        self.l1 = transformers.BertModel.from_pretrained('bert-base-uncased', return_dict=False)\n",
    "        self.l2 = torch.nn.Dropout(0.3)\n",
    "        self.l3 = torch.nn.Linear(768, 29)\n",
    "\n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        _, output_1 = self.l1(ids, attention_mask=mask, token_type_ids=token_type_ids)\n",
    "        output_2 = self.l2(output_1)\n",
    "        output = self.l3(output_2)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next I will create a simple loss function using BCE (binary cross-entory) with Logits Loss from torch. I have added positive weights to encourage the model to select a category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fn(outputs, targets):\n",
    "    pos_weight = torch.full([29], 5)\n",
    "    return torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)(outputs, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "def load_ckp(checkpoint_fpath, model, optimizer):\n",
    "    checkpoint = torch.load(checkpoint_fpath)\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "    optimizer.load_state_dict(checkpoint[\"optimizer\"])\n",
    "    valid_loss_min = checkpoint[\"valid_loss_min\"]\n",
    "    return model, optimizer, checkpoint[\"epoch\"], valid_loss_min\n",
    "\n",
    "def save_ckp(state, is_best, checkpoint_path, best_model_path):\n",
    "    torch.save(state, checkpoint_path)\n",
    "    if is_best:\n",
    "        shutil.copyfile(checkpoint_path, best_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting the device object for using in torch training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): # check for CUDA gpu\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available(): # Check for Apple M1/M2 chip\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # Otherwise just use CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "        start_epochs,\n",
    "        n_epochs,\n",
    "        valid_loss_min_input,\n",
    "        training_loader,\n",
    "        validation_loader,\n",
    "        model,\n",
    "        optimizer,\n",
    "        checkpoint_path,\n",
    "        best_model_path\n",
    "):\n",
    "    # Initiialize valid loss minimum at input.\n",
    "    valid_loss_min = valid_loss_min_input\n",
    "\n",
    "    for epoch in range(start_epochs, n_epochs):\n",
    "        train_loss = 0\n",
    "        valid_loss = 0\n",
    "        # Put model in training mode.\n",
    "        model.train()\n",
    "\n",
    "        print(f\" -- Epoch {epoch}: Training Start -- \")\n",
    "        \n",
    "        for batch_idx, data in enumerate(training_loader):\n",
    "            # Save batch info to device\n",
    "            ids = data[\"ids\"].to(device, dtype=torch.long)\n",
    "            mask = data[\"mask\"].to(device, dtype=torch.long)\n",
    "            token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "            targets = data[\"targets\"].to(device, dtype=torch.float)\n",
    "            # Run prediction on model for batch\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            optimizer.zero_grad()\n",
    "            # Evaluate loss\n",
    "            loss = loss_fn(outputs, targets)\n",
    "            \n",
    "            if batch_idx%5000 == 0:\n",
    "                print(f\"Epoch: {epoch}, Training Loss: {loss.item()}\")\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss += (1 / (batch_idx + 1))*(loss.item() - train_loss)\n",
    "        \n",
    "        print(f\" -- Epoch {epoch}: Training End -- \")\n",
    "\n",
    "        print(f\" -- Epoch {epoch}: Validation Start -- \")\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            val_targets = []\n",
    "            val_outputs = []\n",
    "            for batch_idx, data in enumerate(training_loader):\n",
    "                # Save batch info to device\n",
    "                ids = data[\"ids\"].to(device, dtype=torch.long)\n",
    "                mask = data[\"mask\"].to(device, dtype=torch.long)\n",
    "                token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "                targets = data[\"targets\"].to(device, dtype=torch.float)\n",
    "                # Evalutate model on batch\n",
    "                outputs = model(ids, mask, token_type_ids)\n",
    "\n",
    "                loss = loss_fn(outputs, targets)\n",
    "                valid_loss += (1 / (batch_idx + 1))*(loss.item() - valid_loss)\n",
    "                val_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "                val_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "\n",
    "        print(f\" -- Epoch {epoch}: Validation End --\")\n",
    "\n",
    "        train_loss = train_loss/len(training_loader)\n",
    "        valid_loss = valid_loss/len(validation_loader)\n",
    "\n",
    "        print(f\"Epoch: {epoch}\\n\\tAverage Training Loss: {train_loss}\\n\\tAverage Validation Loss: {valid_loss}\")\n",
    "\n",
    "        checkpoint = {\n",
    "            \"epoch\": epoch + 1, \n",
    "            \"valid_loss_min\": valid_loss,\n",
    "            \"state_dict\": model.state_dict(),\n",
    "            \"optimizer\": optimizer.state_dict()\n",
    "        }\n",
    "\n",
    "        save_ckp(checkpoint, False, checkpoint_path, best_model_path)\n",
    "\n",
    "        if valid_loss <= valid_loss_min:\n",
    "            print(f\"Validation loss decreased ({valid_loss_min} --> {valid_loss}). Saving Model...\")\n",
    "            save_ckp(checkpoint, True, checkpoint_path, best_model_path)\n",
    "            valid_loss_min = valid_loss\n",
    "\n",
    "        print(f\" -- Epoch {epoch} Done -- \")\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load in the data we prepared and split out earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 32\n",
    "VALID_BATCH_SIZE = 32\n",
    "LEARNING_RATE = 1e-05\n",
    "\n",
    "training_loader = df_loader(train_dataset, TRAIN_BATCH_SIZE)\n",
    "validation_loader = df_loader(valid_dataset, VALID_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define the key components to begin training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"./current_checkpoint.pt\"\n",
    "best_model = \"./best_model.pt\"\n",
    "model = BERTClass()\n",
    "model.to(device)\n",
    "valid_loss_min_input = np.Int\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That leaves just one thing left to do..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model = train_model(1, 4, valid_loss_min_input, training_loader, validation_loader, model, optimizer, checkpoint_path, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that training is complete on the initial batch there should be a best_model.pt file availble in the directory. We can load that model in using the checkpoint function above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')\n",
    "model = BERTClass()\n",
    "model.to(device)\n",
    "optimizer = torch.optim.Adam(params=model.parameters(), lr=LEARNING_RATE)\n",
    "best_model = \"./best_model.pt\"\n",
    "\n",
    "model, optimizer, epoch, valid_loss_min_input = load_ckp(best_model, model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can load in the test dataset as saved previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"observations-finaltest.csv\")\n",
    "display(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's grab some of the code I wrote above to make a function that preps the dataframe for testing or training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_prep(dataset, category_list):\n",
    "    # Get important fields\n",
    "    df = dataset[[\"whatdidyousee\", \"category\"]]\n",
    "\n",
    "    # Removing Null categories to get labelled list.\n",
    "    df = df.loc[df[\"category\"].notnull()]\n",
    "    df = df.reset_index(drop=True)\n",
    "\n",
    "    # One-hot encoding of categories\n",
    "    for category in category_list:\n",
    "        df.loc[df[\"category\"] == category, category] = 1\n",
    "        df[category] = df[category].fillna(0)\n",
    "\n",
    "    # Organise columns into correctly named fields.\n",
    "    df[\"input\"] = df[\"whatdidyousee\"].astype(str)\n",
    "    df[\"target_list\"] = df[category_list].astype(bool).values.to_list()\n",
    "    return df[[\"input\", \"category\", \"target_list\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SIZE = 1000\n",
    "test_data = test_data.sample(TEST_SIZE).reset_index(drop=True)\n",
    "test_data = feature_prep(test_data, category_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = df_loader(test_data, VALID_BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_validation(dataloader):\n",
    "    model.eval()\n",
    "    fin_targets = []\n",
    "    fin_outputs = []\n",
    "    with torch.no_grad():\n",
    "        for _, data in enumerate(dataloader, 0):\n",
    "            ids = data[\"ids\"].to(device, dtype=torch.long)\n",
    "            mask = data[\"mask\"].to(device, dtype=torch.long)\n",
    "            token_type_ids = data[\"token_type_ids\"].to(device, dtype=torch.long)\n",
    "            outputs = model(ids, mask, token_type_ids)\n",
    "            fin_targets.extend(targets.cpu().detach().numpy().tolist())\n",
    "            fin_outputs.extend(torch.sigmoid(outputs).cpu().detach().numpy().tolist())\n",
    "    return fin_outputs, fin_targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I've created a validation function and have the test data loaded into the custom dataset we can run evaluation on the test records. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs, targets = do_validation(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for o in outputs:\n",
    "    for i, x in enumerate(o):\n",
    "        if x < max(o):\n",
    "            o[i] = 0\n",
    "        else:\n",
    "            o[i] = 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

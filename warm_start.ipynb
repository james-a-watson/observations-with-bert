{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Warm Start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this notebook is to take the initial model produced by model_train.ipynb and use the same functionality to train over more labelled records. This will build on the weights that are currently set in the model and further tune them by exposing the model to more data.\n",
    "\n",
    "You should be able to run this over and over again and see better and better results on the test dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch \n",
    "import numpy as np\n",
    "from data_helpers import feature_prep, split_sample\n",
    "from model import load_checkpoint, BERTClass, train_model\n",
    "from transformers import BertTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in Best Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set the path to your favorite model here. Note that models are saved to a warm_start folder to prevent confusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_model = \"best_model.pt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's set our device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.is_available(): # check for CUDA gpu\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available(): #Â Check for Apple M1/M2 chip\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\") # Otherwise just use CPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** Once the optimizer has been through a few epochs the learning rate is so small that no significant improvements are made in the results. To rectify this I have been reseting the optimizer here rather than using the one loaded from the checkpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating some variables\n",
    "EPOCHS = 4\n",
    "LEARNING_RATE = 1e-05\n",
    "\n",
    "checkpoint_path = \"warm_start/current_checkpoint.pt\"\n",
    "best_model = \"warm_start/best_model.pt\"\n",
    "valid_loss_min_input = np.Inf\n",
    "\n",
    "# Inititialising model components\n",
    "model = BERTClass()\n",
    "model.to(device)\n",
    "optimizer_init = torch.optim.Adam()\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Loading in model\n",
    "model, optimizer, epoch, valid_loss_min_input = load_checkpoint(load_model, model, optimizer_init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next few cells we will take a cut of the remaining unseen observations ready for training and remove those recrods from the unseen data to avoid picking them out again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "with open(\"observation_categories.json\", \"r\") as f:\n",
    "    categories = json.load(f)[\"categories\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**NB:** observation_categories.json and observations_unseen.csv should have been created by model_train.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data = pd.read_csv(\"observations-unseen.csv\")\n",
    "print(f\"FULL Dataset: {model.shape}\")\n",
    "display(model_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take a sample of the unseen data that we have loaded in for training our notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_SIZE = 50000\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "sample_data = model_data.sample(SAMPLE_SIZE)\n",
    "sample_data = feature_prep(sample_data)\n",
    "training_loader, validation_loader = split_sample(sample_data, TRAIN_SIZE)\n",
    "\n",
    "model_data = model_data.drop(sample_data.index).reset_index(drop=True)\n",
    "\n",
    "print(f\"REMAINING Dataset: {model_data.shape}\")\n",
    "display(model_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that looks good we can save the unseen dataset back to the CSV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_data.to_csv(\"observations-unseen.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can pull in the training function and run it over the sample dataset with the loaded model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "train_model(\n",
    "        start_epochs,\n",
    "        n_epochs,\n",
    "        valid_loss_min_input,\n",
    "        training_loader,\n",
    "        validation_loader,\n",
    "        model,\n",
    "        device,\n",
    "        optimizer,\n",
    "        checkpoint_path,\n",
    "        best_model_path\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
